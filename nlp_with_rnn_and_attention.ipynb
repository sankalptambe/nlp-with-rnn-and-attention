{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with RNNs and Attention\n",
    "\n",
    "In this notebook we will explore the character RNN, stateless RNN, stateful RNN, RNN for sentimental analysis, Encoder-Decoder architecture and finally Transformer.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -q -U tensorflow-addons\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character RNN\n",
    "\n",
    "### Splitting a sequence into batches of shuffled windows\n",
    "For example, let's split the sequence 0 to 14 into windows of length 5, each shifted by 2 (e.g.,[0, 1, 2, 3, 4], [2, 3, 4, 5, 6], etc.), then shuffle them, and split them into inputs (the first 4 steps) and targets (the last 4 steps) (e.g., [2, 3, 4, 5, 6] would be split into [[2, 3, 4, 5], [3, 4, 5, 6]]), then create batches of 3 such input/target pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0 \n",
      "X_batch\n",
      "[[6 7 8 9]\n",
      " [2 3 4 5]\n",
      " [4 5 6 7]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 7  8  9 10]\n",
      " [ 3  4  5  6]\n",
      " [ 5  6  7  8]]\n",
      "____________________ Batch 1 \n",
      "X_batch\n",
      "[[ 0  1  2  3]\n",
      " [ 8  9 10 11]\n",
      " [10 11 12 13]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 1  2  3  4]\n",
      " [ 9 10 11 12]\n",
      " [11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_steps = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda windows: (windows[:-1], windows[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
    "    print(X_batch.numpy())\n",
    "    print(\"=\" * 5, \"\\nY_batch\")\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char_level => character level encoding. default is word level encoding\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True) \n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['First'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # no. of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than form 1 to 39)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
    "\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Model\n",
    "We first need to preprocess it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprecess(['How are yo'])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating fake Shakespearean text\n",
    "Until now, we would predict the next 'letter' in the text. To generate new text using the Char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it at the end of the text, then give the extended text to the model to guess the next letter, and so on. But in practice this often leads tot he same words being repeated over and over again.\n",
    "\n",
    "Instead we can pick the next character randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_new = preprocess([text])\n",
    "# ... to be continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN\n",
    "In stateless RNN, at each training iteration the model starts with a hidden state full of zeroes, then it updates this state at each time step, and after the last time step, it throws it away, as it is not needed anymore.\n",
    "\n",
    "We can preserve the final state after processing one training batch and use it as the initial state for the next training batch. This way model can learn long-term patterns despite only backpropagating through short sequences. This is called *stateful RNN*.\n",
    "\n",
    "**Note**: stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. Hence we will have to use sequential nonoverlapping input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.repeat().batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of each epoch, we need to reset the states before we go back to the beginning of the text. For this we use a callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "steps_per_epoch = train_size // batch_size // n_steps\n",
    "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentimental Analysis\n",
    "We will use IMDB reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10] # the dataset is already preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate(('<pad>', '<sos>', '<unk>')):\n",
    "    id_to_word[id_] = token\n",
    "    \n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/sankalp/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dd18aad4e24f22bb7e31c15237c4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae2d20bf47b42e5a12b0ea41531ff79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/sankalp/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteB5T8M1/imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2af655b30f14ff4bc9c5dec19143522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/sankalp/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteB5T8M1/imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37aaa2405f844ebdae5b0c8c22054610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/sankalp/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteB5T8M1/imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ad55bf5bce4ab6a6c3038eef1957ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/sankalp/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'unsupervised'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = info.splits['train'].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples\n",
    "\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \") # replacing <br /> tags   \n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z]\", b\" \") # replacing any characters other than a-z\n",
    "    X_batch = tf.strings.split(X_batch) # split by spaces\n",
    "    # converting ragged tensor to dense tensor and padding for same length\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8, 60, 1), dtype=string, numpy=\n",
       " array([[[b'Red'],\n",
       "         [b'Eye'],\n",
       "         [b'is'],\n",
       "         [b'not'],\n",
       "         [b'the'],\n",
       "         [b'kind'],\n",
       "         [b'of'],\n",
       "         [b'movie'],\n",
       "         [b'that'],\n",
       "         [b's'],\n",
       "         [b'going'],\n",
       "         [b'to'],\n",
       "         [b'win'],\n",
       "         [b'the'],\n",
       "         [b'Palme'],\n",
       "         [b'D'],\n",
       "         [b'or'],\n",
       "         [b'but'],\n",
       "         [b'Wes'],\n",
       "         [b'Craven'],\n",
       "         [b'has'],\n",
       "         [b'never'],\n",
       "         [b'been'],\n",
       "         [b'that'],\n",
       "         [b'kind'],\n",
       "         [b'of'],\n",
       "         [b'director'],\n",
       "         [b'anyway'],\n",
       "         [b'and'],\n",
       "         [b'his'],\n",
       "         [b'branding'],\n",
       "         [b'is'],\n",
       "         [b'a'],\n",
       "         [b'good'],\n",
       "         [b'indication'],\n",
       "         [b'of'],\n",
       "         [b'what'],\n",
       "         [b'a'],\n",
       "         [b'film'],\n",
       "         [b'goer'],\n",
       "         [b'can'],\n",
       "         [b'expect'],\n",
       "         [b'The'],\n",
       "         [b'fact'],\n",
       "         [b'that'],\n",
       "         [b'Red'],\n",
       "         [b'Eye'],\n",
       "         [b'is'],\n",
       "         [b'a'],\n",
       "         [b'tight'],\n",
       "         [b'little'],\n",
       "         [b'undemanding'],\n",
       "         [b'package'],\n",
       "         [b'at'],\n",
       "         [b'minutes'],\n",
       "         [b'is'],\n",
       "         [b'part'],\n",
       "         [b'of'],\n",
       "         [b'its'],\n",
       "         [b'pad']],\n",
       " \n",
       "        [[b'What'],\n",
       "         [b'is'],\n",
       "         [b'left'],\n",
       "         [b'of'],\n",
       "         [b'Planet'],\n",
       "         [b'Earth'],\n",
       "         [b'is'],\n",
       "         [b'populated'],\n",
       "         [b'by'],\n",
       "         [b'a'],\n",
       "         [b'few'],\n",
       "         [b'poor'],\n",
       "         [b'and'],\n",
       "         [b'starving'],\n",
       "         [b'rag'],\n",
       "         [b'tag'],\n",
       "         [b'survivors'],\n",
       "         [b'They'],\n",
       "         [b'must'],\n",
       "         [b'eat'],\n",
       "         [b'bugs'],\n",
       "         [b'and'],\n",
       "         [b'insects'],\n",
       "         [b'or'],\n",
       "         [b'whatever'],\n",
       "         [b'after'],\n",
       "         [b'a'],\n",
       "         [b'poison'],\n",
       "         [b'war'],\n",
       "         [b'or'],\n",
       "         [b'something'],\n",
       "         [b'has'],\n",
       "         [b'nearly'],\n",
       "         [b'wiped'],\n",
       "         [b'out'],\n",
       "         [b'all'],\n",
       "         [b'human'],\n",
       "         [b'civilization'],\n",
       "         [b'In'],\n",
       "         [b'these'],\n",
       "         [b'dark'],\n",
       "         [b'times'],\n",
       "         [b'one'],\n",
       "         [b'of'],\n",
       "         [b'the'],\n",
       "         [b'few'],\n",
       "         [b'people'],\n",
       "         [b'on'],\n",
       "         [b'Earth'],\n",
       "         [b'still'],\n",
       "         [b'able'],\n",
       "         [b'to'],\n",
       "         [b'live'],\n",
       "         [b'in'],\n",
       "         [b'co'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad']],\n",
       " \n",
       "        [[b'The'],\n",
       "         [b'last'],\n",
       "         [b'Tarzan'],\n",
       "         [b'film'],\n",
       "         [b'starring'],\n",
       "         [b'Johnny'],\n",
       "         [b'Weissmuller'],\n",
       "         [b'looking'],\n",
       "         [b'surprisingly'],\n",
       "         [b'aged'],\n",
       "         [b'a'],\n",
       "         [b'year'],\n",
       "         [b'after'],\n",
       "         [b'Tarzan'],\n",
       "         [b'and'],\n",
       "         [b'the'],\n",
       "         [b'Huntress'],\n",
       "         [b'is'],\n",
       "         [b'bad'],\n",
       "         [b'in'],\n",
       "         [b'spite'],\n",
       "         [b'of'],\n",
       "         [b'all'],\n",
       "         [b'the'],\n",
       "         [b'trivia'],\n",
       "         [b'one'],\n",
       "         [b'can'],\n",
       "         [b'add'],\n",
       "         [b'to'],\n",
       "         [b'make'],\n",
       "         [b'it'],\n",
       "         [b'look'],\n",
       "         [b'better'],\n",
       "         [b'It'],\n",
       "         [b'is'],\n",
       "         [b'obvious'],\n",
       "         [b'that'],\n",
       "         [b'RKO'],\n",
       "         [b'tried'],\n",
       "         [b'to'],\n",
       "         [b'make'],\n",
       "         [b'a'],\n",
       "         [b'great'],\n",
       "         [b'farewell'],\n",
       "         [b'for'],\n",
       "         [b'Weissmuller'],\n",
       "         [b'shooting'],\n",
       "         [b'in'],\n",
       "         [b'beautiful'],\n",
       "         [b'scenery'],\n",
       "         [b'in'],\n",
       "         [b'M'],\n",
       "         [b'xico'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad']],\n",
       " \n",
       "        [[b'I'],\n",
       "         [b'have'],\n",
       "         [b'a'],\n",
       "         [b'severe'],\n",
       "         [b'problem'],\n",
       "         [b'with'],\n",
       "         [b'this'],\n",
       "         [b'show'],\n",
       "         [b'several'],\n",
       "         [b'actually'],\n",
       "         [b'A'],\n",
       "         [b'simple'],\n",
       "         [b'list'],\n",
       "         [b'will'],\n",
       "         [b'suffice'],\n",
       "         [b'for'],\n",
       "         [b'now'],\n",
       "         [b'I'],\n",
       "         [b'll'],\n",
       "         [b'go'],\n",
       "         [b'into'],\n",
       "         [b'more'],\n",
       "         [b'depth'],\n",
       "         [b'later'],\n",
       "         [b'on'],\n",
       "         [b'superficial'],\n",
       "         [b'characters'],\n",
       "         [b'a'],\n",
       "         [b'laugh'],\n",
       "         [b'track'],\n",
       "         [b'and'],\n",
       "         [b'boring'],\n",
       "         [b'humour'],\n",
       "         [b'If'],\n",
       "         [b'you'],\n",
       "         [b'don'],\n",
       "         [b't'],\n",
       "         [b'wish'],\n",
       "         [b'to'],\n",
       "         [b'look'],\n",
       "         [b'at'],\n",
       "         [b'the'],\n",
       "         [b'rest'],\n",
       "         [b'of'],\n",
       "         [b'this'],\n",
       "         [b'review'],\n",
       "         [b'and'],\n",
       "         [b'are'],\n",
       "         [b'only'],\n",
       "         [b'reading'],\n",
       "         [b'it'],\n",
       "         [b'so'],\n",
       "         [b'you'],\n",
       "         [b'can'],\n",
       "         [b'feel'],\n",
       "         [b'superior'],\n",
       "         [b'a'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad']],\n",
       " \n",
       "        [[b'The'],\n",
       "         [b'year'],\n",
       "         [b'is'],\n",
       "         [b'Ernesto'],\n",
       "         [b'Che'],\n",
       "         [b'Guevara'],\n",
       "         [b'having'],\n",
       "         [b'been'],\n",
       "         [b'a'],\n",
       "         [b'Cuban'],\n",
       "         [b'citizen'],\n",
       "         [b'for'],\n",
       "         [b'the'],\n",
       "         [b'last'],\n",
       "         [b'five'],\n",
       "         [b'years'],\n",
       "         [b'disappears'],\n",
       "         [b'from'],\n",
       "         [b'the'],\n",
       "         [b'face'],\n",
       "         [b'of'],\n",
       "         [b'the'],\n",
       "         [b'Earth'],\n",
       "         [b'leaving'],\n",
       "         [b'a'],\n",
       "         [b'glum'],\n",
       "         [b'Fidel'],\n",
       "         [b'Castro'],\n",
       "         [b'to'],\n",
       "         [b'announce'],\n",
       "         [b'that'],\n",
       "         [b'he'],\n",
       "         [b'is'],\n",
       "         [b'probably'],\n",
       "         [b'dead'],\n",
       "         [b'when'],\n",
       "         [b'in'],\n",
       "         [b'truth'],\n",
       "         [b'he'],\n",
       "         [b'has'],\n",
       "         [b'left'],\n",
       "         [b'Cuba'],\n",
       "         [b'to'],\n",
       "         [b'move'],\n",
       "         [b'to'],\n",
       "         [b'Bolivia'],\n",
       "         [b'to'],\n",
       "         [b'live'],\n",
       "         [b'an'],\n",
       "         [b'assumed'],\n",
       "         [b'identity'],\n",
       "         [b'Whilst'],\n",
       "         [b'living'],\n",
       "         [b'in'],\n",
       "         [b'La'],\n",
       "         [b'Paz'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad']],\n",
       " \n",
       "        [[b'Okay'],\n",
       "         [b'So'],\n",
       "         [b'I'],\n",
       "         [b'just'],\n",
       "         [b'got'],\n",
       "         [b'back'],\n",
       "         [b'Before'],\n",
       "         [b'I'],\n",
       "         [b'start'],\n",
       "         [b'my'],\n",
       "         [b'review'],\n",
       "         [b'let'],\n",
       "         [b'me'],\n",
       "         [b'tell'],\n",
       "         [b'you'],\n",
       "         [b'one'],\n",
       "         [b'thing'],\n",
       "         [b'I'],\n",
       "         [b'wanted'],\n",
       "         [b'to'],\n",
       "         [b'like'],\n",
       "         [b'this'],\n",
       "         [b'movie'],\n",
       "         [b'I'],\n",
       "         [b'know'],\n",
       "         [b'I'],\n",
       "         [b've'],\n",
       "         [b'been'],\n",
       "         [b'negative'],\n",
       "         [b'in'],\n",
       "         [b'the'],\n",
       "         [b'past'],\n",
       "         [b'but'],\n",
       "         [b'I'],\n",
       "         [b'was'],\n",
       "         [b'hoping'],\n",
       "         [b'to'],\n",
       "         [b'be'],\n",
       "         [b'surprised'],\n",
       "         [b'and'],\n",
       "         [b'actually'],\n",
       "         [b'come'],\n",
       "         [b'out'],\n",
       "         [b'liking'],\n",
       "         [b'the'],\n",
       "         [b'film'],\n",
       "         [b'I'],\n",
       "         [b'didn'],\n",
       "         [b't'],\n",
       "         [b'It'],\n",
       "         [b's'],\n",
       "         [b'not'],\n",
       "         [b'just'],\n",
       "         [b'the'],\n",
       "         [b'fact'],\n",
       "         [b'that'],\n",
       "         [b'every'],\n",
       "         [b'horror'],\n",
       "         [b'clich'],\n",
       "         [b'imaginable']],\n",
       " \n",
       "        [[b'When'],\n",
       "         [b'I'],\n",
       "         [b'saw'],\n",
       "         [b'this'],\n",
       "         [b'trailer'],\n",
       "         [b'on'],\n",
       "         [b'TV'],\n",
       "         [b'I'],\n",
       "         [b'was'],\n",
       "         [b'surprised'],\n",
       "         [b'In'],\n",
       "         [b'May'],\n",
       "         [b'of'],\n",
       "         [b'I'],\n",
       "         [b'was'],\n",
       "         [b'at'],\n",
       "         [b'Six'],\n",
       "         [b'Flags'],\n",
       "         [b'in'],\n",
       "         [b'New'],\n",
       "         [b'Jersey'],\n",
       "         [b'and'],\n",
       "         [b'this'],\n",
       "         [b'was'],\n",
       "         [b'showing'],\n",
       "         [b'at'],\n",
       "         [b'a'],\n",
       "         [b'D'],\n",
       "         [b'attraction'],\n",
       "         [b'you'],\n",
       "         [b'know'],\n",
       "         [b'the'],\n",
       "         [b'attraction'],\n",
       "         [b'that'],\n",
       "         [b'the'],\n",
       "         [b'seats'],\n",
       "         [b'move'],\n",
       "         [b'I'],\n",
       "         [b'take'],\n",
       "         [b'it'],\n",
       "         [b'that'],\n",
       "         [b'the'],\n",
       "         [b'version'],\n",
       "         [b'I'],\n",
       "         [b'saw'],\n",
       "         [b'was'],\n",
       "         [b'a'],\n",
       "         [b'shortened'],\n",
       "         [b'version'],\n",
       "         [b'min'],\n",
       "         [b'and'],\n",
       "         [b'also'],\n",
       "         [b're'],\n",
       "         [b'created'],\n",
       "         [b'to'],\n",
       "         [b'add'],\n",
       "         [b'the'],\n",
       "         [b'motion'],\n",
       "         [b'effects'],\n",
       "         [b'It']],\n",
       " \n",
       "        [[b'First'],\n",
       "         [b'of'],\n",
       "         [b'all'],\n",
       "         [b'Riget'],\n",
       "         [b'is'],\n",
       "         [b'wonderful'],\n",
       "         [b'Good'],\n",
       "         [b'comedy'],\n",
       "         [b'and'],\n",
       "         [b'mystery'],\n",
       "         [b'thriller'],\n",
       "         [b'at'],\n",
       "         [b'the'],\n",
       "         [b'same'],\n",
       "         [b'time'],\n",
       "         [b'Nice'],\n",
       "         [b'combination'],\n",
       "         [b'of'],\n",
       "         [b'strange'],\n",
       "         [b'dogma'],\n",
       "         [b'style'],\n",
       "         [b'of'],\n",
       "         [b'telling'],\n",
       "         [b'the'],\n",
       "         [b'story'],\n",
       "         [b'together'],\n",
       "         [b'with'],\n",
       "         [b'good'],\n",
       "         [b'music'],\n",
       "         [b'and'],\n",
       "         [b'great'],\n",
       "         [b'actors'],\n",
       "         [b'But'],\n",
       "         [b'unfortunately'],\n",
       "         [b'there'],\n",
       "         [b's'],\n",
       "         [b'no'],\n",
       "         [b'the'],\n",
       "         [b'end'],\n",
       "         [b'As'],\n",
       "         [b'for'],\n",
       "         [b'me'],\n",
       "         [b'it'],\n",
       "         [b's'],\n",
       "         [b'unacceptable'],\n",
       "         [b'I'],\n",
       "         [b'was'],\n",
       "         [b'thinking'],\n",
       "         [b'how'],\n",
       "         [b'it'],\n",
       "         [b'will'],\n",
       "         [b'be'],\n",
       "         [b'possibl'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad'],\n",
       "         [b'pad']]], dtype=object)>,\n",
       " <tf.Tensor: shape=(8,), dtype=int64, numpy=array([1, 0, 0, 0, 1, 0, 0, 1])>)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to construct a vocabulary: go through the whole training set once, applying preprocess() function, and using a `Counter` to count the no. of occurrences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets['train'].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 224494), (b'the', 61156), (b'a', 38569)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping only the 10000 common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, adding preprocessing step to replace each word and replacing with its ID, we will create a lookup table using 1000 oov buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   24,    12,    13, 10770]])>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b'This movie was faaaaaaantastic'.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets['train'].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 122s 156ms/step - loss: 0.6713 - accuracy: 0.5507\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 124s 158ms/step - loss: 0.4195 - accuracy: 0.8056\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 118s 151ms/step - loss: 0.2785 - accuracy: 0.8876\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 107s 137ms/step - loss: 0.1882 - accuracy: 0.9311\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 5422s 7s/step - loss: 0.1496 - accuracy: 0.9468\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masking\n",
    "In order to ignore the padding tokens add mask_zero=True when creating Embedding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reusing Pretrained Embeddings\n",
    "The Tensorflow Hub project makes it easy to reuse pretrained model components in your own models. These model components are called *modules*. Browse the [TF Hub repository](https://tfhub.dev)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f97d464a820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x7f97d464a820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7f97d4c96a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x7f97d4c96a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x7f97d464a3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x7f97d464a3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "model = keras.Sequential([\n",
    "    hub.KerasLayer('https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1',\n",
    "                  dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/saved_model.pb\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.data-00000-of-00001\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/variables/variables.index\n",
      "./my_tfhub_cache/82c4aaf4250ffb09088bd48368ee7fd00e5464fe/assets/tokens.txt\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can just load the IMDB review dataset and directly train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "781/781 [==============================] - 120s 154ms/step - loss: 0.5460 - accuracy: 0.7267\n",
      "Epoch 2/5\n",
      "  1/781 [..............................] - ETA: 0s - loss: 0.7954 - accuracy: 0.7500WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3905 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3905 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "  1/781 [..............................] - 0s 304ms/step - loss: 0.7954 - accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\n",
    "train_size = info.splits['train'].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets['train'].batch(batch_size).prefetch(1)\n",
    "\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Encoder-Decoder Network for Neural Machine Translation\n",
    "Translating English sentences to French:\n",
    "\n",
    "##### how it works?\n",
    "The English sentences are fed to the encoder, and the decoder outputs the French transalations. Note that the French translations are also used as inputs to the decoder, but shifted back by one step. In other words, the decoder is given as input the word that it *should* have output at the previous step.\n",
    "\n",
    "Note that the English sentence will be reversed before they are fed to the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "embed_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow Addons project includes many sequence-to-sequence tools to let you easily build production-ready Encoder-Decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 7s 221ms/step - loss: 4.6052\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 7s 212ms/step - loss: 4.6031\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
